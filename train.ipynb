{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42b33964-2cdb-44eb-bacb-040195075183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:12:10.759150Z",
     "iopub.status.busy": "2025-11-21T15:12:10.758415Z",
     "iopub.status.idle": "2025-11-21T15:12:14.092710Z",
     "shell.execute_reply": "2025-11-21T15:12:14.090456Z",
     "shell.execute_reply.started": "2025-11-21T15:12:10.759087Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /opt/conda/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.9/site-packages (4.6.0)\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.9/site-packages (4.6.0)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.9/site-packages (from imblearn) (0.12.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.23.3)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from optuna) (1.8.1)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.9/site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.9/site-packages (from optuna) (1.4.41)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from sqlalchemy>=1.4.2->optuna) (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install imblearn lightgbm optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7b576ca-b28b-45b2-8eeb-ac4c88735846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:12:14.623606Z",
     "iopub.status.busy": "2025-11-21T15:12:14.622916Z",
     "iopub.status.idle": "2025-11-21T15:12:14.638385Z",
     "shell.execute_reply": "2025-11-21T15:12:14.636166Z",
     "shell.execute_reply.started": "2025-11-21T15:12:14.623544Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# --- å¼•å…¥æ¨¡å‹èˆ‡å·¥å…· ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, f1_score,\n",
    "    roc_auc_score, classification_report\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "# --- ç’°å¢ƒè¨­å®š ---\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d933d66-ee58-4b39-9936-3daf9c164510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:12:21.756257Z",
     "iopub.status.busy": "2025-11-21T15:12:21.755921Z",
     "iopub.status.idle": "2025-11-21T15:12:24.747116Z",
     "shell.execute_reply": "2025-11-21T15:12:24.745790Z",
     "shell.execute_reply.started": "2025-11-21T15:12:21.756232Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. è¼‰å…¥å‰è™•ç†å¾Œçš„è³‡æ–™ ---\n",
      "âœ… è³‡æ–™è¼‰å…¥æˆåŠŸã€‚\n",
      "è¨“ç·´é›† (SMOTEå¾Œ) åŸå§‹ç‰¹å¾µæ•¸: 123ï¼Œç­†æ•¸: 101280\n",
      "æ¸¬è©¦é›† åŸå§‹ç‰¹å¾µæ•¸: 123ï¼Œç­†æ•¸: 16266\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ å®šç¾©å„²å­˜æ¨¡å‹äººå·¥è£½å“çš„è·¯å¾‘ï¼Œä¸¦ç¢ºä¿ç›®éŒ„å­˜åœ¨\n",
    "SCALER_SAVE_PATH = \"./model_artifacts\" \n",
    "if not os.path.exists(SCALER_SAVE_PATH):\n",
    "    os.makedirs(SCALER_SAVE_PATH, exist_ok=True)\n",
    "    print(f\"âœ… ç›®éŒ„å·²å‰µå»º: {SCALER_SAVE_PATH}\")\n",
    "\n",
    "\n",
    "FILE_PATHS = {\n",
    "    # è«‹ç¢ºä¿é€™äº› CSV æª”æ¡ˆçš„ç‰¹å¾µæ•¸é‡èˆ‡æ¨¡å‹è¨“ç·´æ™‚çš„ 60 å€‹ç‰¹å¾µä¸€è‡´\n",
    "    'X_train_resampled': 'processed_data/X_train_resampled.csv',\n",
    "    'y_train_resampled': 'processed_data/y_train_resampled.csv',\n",
    "    'X_test': 'processed_data/X_test.csv',\n",
    "    'y_test': 'processed_data/y_test.csv'\n",
    "}\n",
    "print(\"--- 1. è¼‰å…¥å‰è™•ç†å¾Œçš„è³‡æ–™ ---\")\n",
    "try:\n",
    "    # è¼‰å…¥å¹³è¡¡å¾Œçš„è¨“ç·´é›†\n",
    "    X_train = pd.read_csv(FILE_PATHS['X_train_resampled'])\n",
    "    y_train = pd.read_csv(FILE_PATHS['y_train_resampled']).squeeze()\n",
    "    \n",
    "    # è¼‰å…¥æ¸¬è©¦é›†\n",
    "    X_test = pd.read_csv(FILE_PATHS['X_test'])\n",
    "    y_test = pd.read_csv(FILE_PATHS['y_test']).squeeze()\n",
    "    \n",
    "    print(f\"âœ… è³‡æ–™è¼‰å…¥æˆåŠŸã€‚\")\n",
    "    print(f\"è¨“ç·´é›† (SMOTEå¾Œ) åŸå§‹ç‰¹å¾µæ•¸: {X_train.shape[1]}ï¼Œç­†æ•¸: {len(X_train)}\")\n",
    "    print(f\"æ¸¬è©¦é›† åŸå§‹ç‰¹å¾µæ•¸: {X_test.shape[1]}ï¼Œç­†æ•¸: {len(X_test)}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âš ï¸ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {e}ã€‚è«‹ç¢ºèª 'processed_data' è³‡æ–™å¤¾å’Œæª”æ¡ˆè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a1ac8e7-23cd-4d39-9e6a-2db6f26a381d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:12:26.883515Z",
     "iopub.status.busy": "2025-11-21T15:12:26.883167Z",
     "iopub.status.idle": "2025-11-21T15:12:27.000092Z",
     "shell.execute_reply": "2025-11-21T15:12:26.999045Z",
     "shell.execute_reply.started": "2025-11-21T15:12:26.883485Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. è³‡æ–™åˆ‡åˆ† (è¨“ç·´é›† -> æœ€çµ‚è¨“ç·´é›† + ç¨ç«‹é©—è­‰é›†) ---\n",
      "ç”¨æ–¼ GridSearchCV çš„æœ€çµ‚è¨“ç·´é›†ç­†æ•¸: 91152\n",
      "ç¨ç«‹é©—è­‰é›†ç­†æ•¸: 10128\n",
      "ç¨ç«‹æ¸¬è©¦é›†ç­†æ•¸: 16266\n"
     ]
    }
   ],
   "source": [
    "# æ­¥é©Ÿ 2: å»ºç«‹æœ€çµ‚è¨“ç·´é›†èˆ‡ç¨ç«‹é©—è­‰é›† (å®šç¾© X_train_final)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 2. è³‡æ–™åˆ‡åˆ† (è¨“ç·´é›† -> æœ€çµ‚è¨“ç·´é›† + ç¨ç«‹é©—è­‰é›†) ---\")\n",
    "\n",
    "# å¾ SMOTE å¾Œçš„è¨“ç·´é›†ä¸­åˆ‡åˆ† 10% ä½œç‚ºç¨ç«‹çš„é©—è­‰é›†\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"ç”¨æ–¼ GridSearchCV çš„æœ€çµ‚è¨“ç·´é›†ç­†æ•¸: {len(X_train_final)}\")\n",
    "print(f\"ç¨ç«‹é©—è­‰é›†ç­†æ•¸: {len(X_val)}\")\n",
    "print(f\"ç¨ç«‹æ¸¬è©¦é›†ç­†æ•¸: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4a8cc29-6661-4d5d-b892-74e0d91c957a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:12:28.094272Z",
     "iopub.status.busy": "2025-11-21T15:12:28.093873Z",
     "iopub.status.idle": "2025-11-21T15:12:28.340387Z",
     "shell.execute_reply": "2025-11-21T15:12:28.339038Z",
     "shell.execute_reply.started": "2025-11-21T15:12:28.094245Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. æ¨™æº–åŒ– (Scaling) èˆ‡ç‰¹å¾µå°é½Šä¿®æ­£ ---\n",
      "âœ… Scaler èˆ‡ç‰¹å¾µåç¨± (å…± 123 å€‹) å·²å„²å­˜åˆ° ./model_artifactsã€‚\n"
     ]
    }
   ],
   "source": [
    "# æ­¥é©Ÿ 3: æ¨™æº–åŒ– (Scaling) èˆ‡ç‰¹å¾µå°é½Šä¿®æ­£ (é¿å… ValueError)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 3. æ¨™æº–åŒ– (Scaling) èˆ‡ç‰¹å¾µå°é½Šä¿®æ­£ ---\")\n",
    "\n",
    "# **ğŸ¯ æ ¸å¿ƒé‚è¼¯ï¼šç¢ºä¿ X_test æ¬„ä½èˆ‡ X_train_final æ¬„ä½æ•¸é‡å’Œé †åºå®Œå…¨ä¸€è‡´ **\n",
    "train_cols = X_train_final.columns.tolist()\n",
    "original_test_cols = X_test.shape[1]\n",
    "\n",
    "if original_test_cols != len(train_cols):\n",
    "    print(f\"âš ï¸ è­¦å‘Šï¼šæ¸¬è©¦é›†ç‰¹å¾µæ•¸ ({original_test_cols}) èˆ‡è¨“ç·´é›†ç‰¹å¾µæ•¸ ({len(train_cols)}) ä¸ä¸€è‡´ã€‚åŸ·è¡Œå°é½Š...\")\n",
    "    \n",
    "    # åŸ·è¡Œç‰¹å¾µå°é½Šï¼šæ–°å¢è¨“ç·´é›†æœ‰ä½†æ¸¬è©¦é›†æ²’æœ‰çš„æ¬„ä½ï¼Œä¸¦åˆªé™¤æ¸¬è©¦é›†å¤šé¤˜çš„æ¬„ä½\n",
    "    X_test = X_test.reindex(columns=train_cols, fill_value=0)\n",
    "    \n",
    "    if X_test.shape[1] == len(train_cols):\n",
    "         print(f\"âœ… X_test ç‰¹å¾µæ•¸é‡å·²ä¿®æ­£ä¸¦å°é½Šã€‚ä¿®æ­£å¾Œç‰¹å¾µæ•¸: {X_test.shape[1]}\")\n",
    "    else:\n",
    "         # å¦‚æœåˆ°é€™ä¸€æ­¥é‚„ä¸å°é½Šï¼Œé‚£èªªæ˜åŸå§‹è³‡æ–™é›†æœ‰åš´é‡å•é¡Œ\n",
    "         print(f\"âŒ éŒ¯èª¤ï¼šç‰¹å¾µå°é½Šå¤±æ•—ï¼Œè«‹æª¢æŸ¥åŸå§‹ CSV æª”æ¡ˆçš„å…§å®¹ã€‚\")\n",
    "         exit() # å¦‚æœç„¡æ³•å°é½Šï¼Œå‰‡çµ‚æ­¢ç¨‹å¼é¿å…éŒ¯èª¤çš„è¨“ç·´\n",
    "         \n",
    "# å»ºç«‹ Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# åƒ…ä½¿ç”¨æœ€çµ‚è¨“ç·´é›† X_train_final ä¾† fit Scaler\n",
    "X_train_final_scaled = pd.DataFrame(scaler.fit_transform(X_train_final), columns=train_cols)\n",
    "\n",
    "# ä½¿ç”¨ fit å¥½çš„ Scaler ä¾† transform é©—è­‰é›†å’Œæ¸¬è©¦é›†\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=train_cols)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=train_cols) # ä½¿ç”¨ä¿®æ­£å¾Œçš„ X_test\n",
    "\n",
    "# å„²å­˜é‡è¦å·¥å…· (Scaler å’Œ æ¬„ä½åç¨±)\n",
    "joblib.dump(scaler, os.path.join(SCALER_SAVE_PATH, \"scaler.pkl\"))\n",
    "joblib.dump(train_cols, os.path.join(SCALER_SAVE_PATH, \"feature_names.pkl\"))\n",
    "print(f\"âœ… Scaler èˆ‡ç‰¹å¾µåç¨± (å…± {len(train_cols)} å€‹) å·²å„²å­˜åˆ° {SCALER_SAVE_PATH}ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec3225e3-71db-4883-8bdc-7915319259e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:12:37.581998Z",
     "iopub.status.busy": "2025-11-21T15:12:37.581234Z",
     "iopub.status.idle": "2025-11-21T15:22:42.004005Z",
     "shell.execute_reply": "2025-11-21T15:22:42.002623Z",
     "shell.execute_reply.started": "2025-11-21T15:12:37.581940Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. æ¨¡å‹è¨“ç·´èˆ‡ GridSearchCV èª¿æ ¡ (ä½¿ç”¨ F2-Score å„ªå…ˆå„ªåŒ–å¬å›ç‡) ---\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "\n",
      "--- è¨“ç·´èˆ‡èª¿æ ¡çµæœå½™å ± ---\n",
      "ğŸ¥‡ æœ€ä½³è¶…åƒæ•¸çµ„åˆ: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "ç¸½è¨“ç·´èˆ‡èª¿æ ¡æ™‚é–“: 601.28 ç§’\n",
      "\n",
      "--- 5. é©—è­‰æ­¥é©Ÿï¼šæ¨¡å‹æ–¼ç¨ç«‹é©—è­‰é›†ä¸Šè©•ä¼° (é è¨­é–¾å€¼ 0.5) ---\n",
      "ğŸ” é©—è­‰é›† æº–ç¢ºç‡ (Accuracy): 0.9109\n",
      "ğŸ” é©—è­‰é›† F1-Score: 0.9111\n",
      "ğŸ” é©—è­‰é›† AUC Score: 0.9738\n",
      "\n",
      "--- 6. æ±ºç­–é–¾å€¼å„ªåŒ–ï¼šåœ¨é©—è­‰é›†ä¸Šå°‹æ‰¾æœ€ä½³é–¾å€¼ (æœ€å¤§åŒ– Rain é¡åˆ¥ F1-Score) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å°‹æ‰¾æœ€ä½³é–¾å€¼: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 162/162 [00:00<00:00, 331.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¥‡ æœ€ä½³ F1-Score (Rain é¡åˆ¥) æ˜¯ 0.9117ï¼Œå°æ‡‰çš„æœ€ä½³é–¾å€¼æ˜¯ 0.515\n",
      "\n",
      "--- 7. æœ€çµ‚è©•ä¼°ï¼šæ¨¡å‹æ–¼æ¸¬è©¦é›†ä¸Šè©•ä¼° (ä½¿ç”¨æœ€ä½³é–¾å€¼) ---\n",
      "âœ… æº–ç¢ºç‡ (Accuracy): 0.8543\n",
      "âœ… åŠ æ¬Š F1-Score: 0.8517\n",
      "âœ… AUC Score (é–¾å€¼ä¸å½±éŸ¿): 0.8900\n",
      "\n",
      "--- æ··æ·†çŸ©é™£ (Confusion Matrix) ---\n",
      "[[TN, FP] (TN=ä¸ä¸‹é›¨æ­£ç¢º, FP=èª¤å ±ä¸‹é›¨)\n",
      " [FN, TP]] (FN=æ¼å ±ä¸‹é›¨, TP=ä¸‹é›¨æ­£ç¢º)\n",
      "[[11641  1019]\n",
      " [ 1351  2255]]\n",
      "\n",
      "--- åˆ†é¡å ±å‘Š (Classification Report) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Rain (0)       0.90      0.92      0.91     12660\n",
      "    Rain (1)       0.69      0.63      0.66      3606\n",
      "\n",
      "    accuracy                           0.85     16266\n",
      "   macro avg       0.79      0.77      0.78     16266\n",
      "weighted avg       0.85      0.85      0.85     16266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.metrics import f1_score, fbeta_score # ğŸ¯ å¼•å…¥ fbeta_score (ç”¨æ–¼ F2-Score)\n",
    "from tqdm import tqdm # å¼•å…¥ tqdm å¢åŠ é€²åº¦æ¢\n",
    "\n",
    "# --- å¼•å…¥æ¨¡å‹èˆ‡å·¥å…· ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, f1_score, fbeta_score, # ç¢ºä¿ fbeta_score è¢«å¼•å…¥\n",
    "    roc_auc_score, classification_report, make_scorer\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# æ­¥é©Ÿ 4: æ¨¡å‹è¨“ç·´èˆ‡è¶…åƒæ•¸èª¿æ ¡ (GridSearchCV) - å„ªå…ˆå„ªåŒ–å¬å›ç‡ (F2-Score)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 4. æ¨¡å‹è¨“ç·´èˆ‡ GridSearchCV èª¿æ ¡ (ä½¿ç”¨ F2-Score å„ªå…ˆå„ªåŒ–å¬å›ç‡) ---\")\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# ğŸ¯ é€²ä¸€æ­¥èª¿æ•´ï¼šæ“´å¤§ min_samples_leaf ç¯„åœï¼Œå˜—è©¦æ›´å¤šæ³›åŒ–èƒ½åŠ›çµ„åˆ\n",
    "param_grid = {\n",
    "    'n_estimators': [150, 300],          \n",
    "    'max_depth': [25, 30, 35],           \n",
    "    'min_samples_leaf': [1, 5, 10],      \n",
    "    'criterion': ['gini', 'entropy']     \n",
    "}\n",
    "\n",
    "# ğŸ¯ æ ¸å¿ƒè®Šæ›´ï¼šä½¿ç”¨ F2-Score (beta=2) ä½œç‚ºå„ªåŒ–ç›®æ¨™ã€‚\n",
    "# F2-Score è³¦äºˆå¬å›ç‡ (Recall) æ›´é«˜çš„æ¬Šé‡ï¼Œç›®æ¨™æ˜¯æ¸›å°‘æ¼å ± (FN)ï¼Œè®“æ¨¡å‹æ›´èƒ½æˆåŠŸæ•æ‰ä¸‹é›¨äº‹ä»¶ã€‚\n",
    "f2_scorer = make_scorer(fbeta_score, pos_label=1, beta=2)\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model, \n",
    "    param_grid=param_grid, \n",
    "    scoring=f2_scorer,      # *** æ›´æ”¹ï¼šä½¿ç”¨ F2-score (é‡å° Rain é¡åˆ¥) ä½œç‚ºä¸»è¦å„ªåŒ–ç›®æ¨™ ***\n",
    "    cv=3,                   \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# ä½¿ç”¨å¹³è¡¡ä¸”æ¨™æº–åŒ–å¾Œçš„æœ€çµ‚è¨“ç·´é›†é€²è¡Œè¨“ç·´èˆ‡äº¤å‰é©—è­‰\n",
    "grid_search.fit(X_train_final_scaled, y_train_final)\n",
    "end_time = time.time()\n",
    "\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "print(\"\\n--- è¨“ç·´èˆ‡èª¿æ ¡çµæœå½™å ± ---\")\n",
    "print(f\"ğŸ¥‡ æœ€ä½³è¶…åƒæ•¸çµ„åˆ: {grid_search.best_params_}\")\n",
    "print(f\"ç¸½è¨“ç·´èˆ‡èª¿æ ¡æ™‚é–“: {end_time - start_time:.2f} ç§’\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# æ­¥é©Ÿ 5: é©—è­‰æ­¥é©Ÿï¼šæ¨¡å‹æ–¼ç¨ç«‹é©—è­‰é›†ä¸Šè©•ä¼° (é è¨­é–¾å€¼ 0.5)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 5. é©—è­‰æ­¥é©Ÿï¼šæ¨¡å‹æ–¼ç¨ç«‹é©—è­‰é›†ä¸Šè©•ä¼° (é è¨­é–¾å€¼ 0.5) ---\")\n",
    "# ä½¿ç”¨æœ€ä½³æ¨¡å‹å°é©—è­‰é›†é€²è¡Œé æ¸¬ (ä½¿ç”¨æ¨™æº–åŒ–å¾Œçš„ X_val_scaled)\n",
    "y_val_pred_default = best_rf_model.predict(X_val_scaled)\n",
    "y_val_proba = best_rf_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred_default)\n",
    "val_f1 = f1_score(y_val, y_val_pred_default)\n",
    "val_auc = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "print(f\"ğŸ” é©—è­‰é›† æº–ç¢ºç‡ (Accuracy): {val_accuracy:.4f}\")\n",
    "print(f\"ğŸ” é©—è­‰é›† F1-Score: {val_f1:.4f}\")\n",
    "print(f\"ğŸ” é©—è­‰é›† AUC Score: {val_auc:.4f}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# æ­¥é©Ÿ 6: æ±ºç­–é–¾å€¼å„ªåŒ– (Threshold Optimization) - èª¿æ•´æ­¥é•·\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 6. æ±ºç­–é–¾å€¼å„ªåŒ–ï¼šåœ¨é©—è­‰é›†ä¸Šå°‹æ‰¾æœ€ä½³é–¾å€¼ (æœ€å¤§åŒ– Rain é¡åˆ¥ F1-Score) ---\")\n",
    "\n",
    "best_f1_rain = 0\n",
    "optimal_threshold = 0.5\n",
    "\n",
    "# ğŸ¯ èª¿æ•´æœç´¢æ­¥é•·ç‚º 0.005ï¼Œå°‹æ‰¾æ›´ç²¾ç¢ºçš„é–¾å€¼\n",
    "thresholds = np.arange(0.10, 0.91, 0.005)\n",
    "\n",
    "# ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢\n",
    "for threshold in tqdm(thresholds, desc=\"å°‹æ‰¾æœ€ä½³é–¾å€¼\"):\n",
    "    # ä½¿ç”¨ç•¶å‰é–¾å€¼é€²è¡Œé æ¸¬\n",
    "    y_val_pred_thresh = (y_val_proba >= threshold).astype(int)\n",
    "    \n",
    "    # è¨ˆç®—ã€ŒRain (1)ã€é¡åˆ¥çš„ F1-Score (é€™è£¡ä»ä½¿ç”¨ F1 ä½œç‚ºæœ€çµ‚éƒ¨ç½²çš„å¹³è¡¡æŒ‡æ¨™)\n",
    "    f1_rain_class = f1_score(y_val, y_val_pred_thresh, pos_label=1)\n",
    "    \n",
    "    if f1_rain_class > best_f1_rain:\n",
    "        best_f1_rain = f1_rain_class\n",
    "        optimal_threshold = threshold\n",
    "\n",
    "print(f\"\\nğŸ¥‡ æœ€ä½³ F1-Score (Rain é¡åˆ¥) æ˜¯ {best_f1_rain:.4f}ï¼Œå°æ‡‰çš„æœ€ä½³é–¾å€¼æ˜¯ {optimal_threshold:.3f}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# æ­¥é©Ÿ 7: æœ€çµ‚æ¨¡å‹æ•ˆèƒ½è©•ä¼° (ä½¿ç”¨æœ€ä½³é–¾å€¼æ–¼æ¸¬è©¦é›†)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 7. æœ€çµ‚è©•ä¼°ï¼šæ¨¡å‹æ–¼æ¸¬è©¦é›†ä¸Šè©•ä¼° (ä½¿ç”¨æœ€ä½³é–¾å€¼) ---\")\n",
    "# ä½¿ç”¨æ¸¬è©¦é›†ä¸Šçš„æ¦‚ç‡\n",
    "y_test_proba = best_rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# æ ¹æ“šå„ªåŒ–å¾Œçš„é–¾å€¼é€²è¡Œæœ€çµ‚é æ¸¬\n",
    "y_test_pred_optimized = (y_test_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# è¨ˆç®—æŒ‡æ¨™\n",
    "accuracy = accuracy_score(y_test, y_test_pred_optimized)\n",
    "f1 = f1_score(y_test, y_test_pred_optimized, average='weighted') # ä½¿ç”¨åŠ æ¬Šå¹³å‡F1ä½œç‚ºç¸½é«”æŒ‡æ¨™\n",
    "cm = confusion_matrix(y_test, y_test_pred_optimized)\n",
    "class_report = classification_report(y_test, y_test_pred_optimized, target_names=['No Rain (0)', 'Rain (1)'])\n",
    "\n",
    "print(f\"âœ… æº–ç¢ºç‡ (Accuracy): {accuracy:.4f}\")\n",
    "print(f\"âœ… åŠ æ¬Š F1-Score: {f1:.4f}\")\n",
    "print(f\"âœ… AUC Score (é–¾å€¼ä¸å½±éŸ¿): {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
    "\n",
    "print(\"\\n--- æ··æ·†çŸ©é™£ (Confusion Matrix) ---\")\n",
    "print(f\"[[TN, FP] (TN=ä¸ä¸‹é›¨æ­£ç¢º, FP=èª¤å ±ä¸‹é›¨)\\n [FN, TP]] (FN=æ¼å ±ä¸‹é›¨, TP=ä¸‹é›¨æ­£ç¢º)\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n--- åˆ†é¡å ±å‘Š (Classification Report) ---\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99831eb1-4365-404f-b807-a04890a27eae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:22:42.005169Z",
     "iopub.status.busy": "2025-11-21T15:22:42.004985Z",
     "iopub.status.idle": "2025-11-21T15:22:42.934777Z",
     "shell.execute_reply": "2025-11-21T15:22:42.932866Z",
     "shell.execute_reply.started": "2025-11-21T15:22:42.005149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. æ¨¡å‹å„²å­˜ ---\n",
      "âœ¨ æœ€ä½³éš¨æ©Ÿæ£®æ—æ¨¡å‹å·²å„²å­˜è‡³ï¼šrandom_forest_final_model.pth\n",
      "âœ¨ StandardScaler èˆ‡ç‰¹å¾µåç¨±å·²å„²å­˜è‡³ï¼š./model_artifacts ç›®éŒ„\n"
     ]
    }
   ],
   "source": [
    "# æ­¥é©Ÿ 7: å„²å­˜æœ€çµ‚æ¨¡å‹ (ä¾›éƒ¨ç½²ä½¿ç”¨)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 7. æ¨¡å‹å„²å­˜ ---\")\n",
    "\n",
    "# ä½¿ç”¨ joblib å„²å­˜æœ€ä½³æ¨¡å‹ (å‰¯æª”å .pth æ˜¯æ ¹æ“šæ‚¨çš„è¦æ±‚è¨­å®š)\n",
    "model_filename = 'random_forest_final_model.pth' \n",
    "joblib.dump(best_rf_model, model_filename)\n",
    "\n",
    "print(f\"âœ¨ æœ€ä½³éš¨æ©Ÿæ£®æ—æ¨¡å‹å·²å„²å­˜è‡³ï¼š{model_filename}\")\n",
    "print(f\"âœ¨ StandardScaler èˆ‡ç‰¹å¾µåç¨±å·²å„²å­˜è‡³ï¼š{SCALER_SAVE_PATH} ç›®éŒ„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8283c88-9521-493b-bbda-c2955cfb9659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:22:42.937288Z",
     "iopub.status.busy": "2025-11-21T15:22:42.936793Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹èˆ‡å·¥å…·è¼‰å…¥æˆåŠŸï¼\n",
      "æ¨¡å‹é æœŸ 123 å€‹ç‰¹å¾µã€‚\n",
      "\n",
      "========================================\n",
      "  è«‹è¼¸å…¥æ°£è±¡æ•¸æ“šé€²è¡Œé™é›¨é æ¸¬  \n",
      "========================================\n",
      "\n",
      "--- æ•¸å€¼ç‰¹å¾µè¼¸å…¥ ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# å¿½ç•¥è­¦å‘Š\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- æ¨¡å‹æª”æ¡ˆè·¯å¾‘è¨­å®š ---\n",
    "MODEL_PATH = 'random_forest_final_model.pth' \n",
    "SCALER_PATH = './model_artifacts/scaler.pkl'\n",
    "FEATURES_PATH = './model_artifacts/feature_names.pkl'\n",
    "# ğŸ¥‡ æœ€ä½³æ±ºç­–é–¾å€¼ (æ ¹æ“š F2 å„ªåŒ–å¾Œçš„çµæœ 0.515)\n",
    "OPTIMAL_THRESHOLD = 0.515 \n",
    "\n",
    "# --- ç‰¹å¾µå®šç¾© (èˆ‡è¨“ç·´æ¨¡å‹ä¿æŒä¸€è‡´) ---\n",
    "NUMERICAL_FEATURES = [\n",
    "    'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', \n",
    "    'WindGustSpeed', 'WindSpeed9am', 'Humidity9am'\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'Location', 'WindDir9am', 'Month'\n",
    "]\n",
    "CATEGORY_CHOICES = {\n",
    "    'Location': ['Sydney', 'Melbourne', 'Perth', 'Darwin', 'Canberra', 'Brisbane', 'Adelaide', 'Hobart', 'Albury', 'WaggaWagga', 'å…¶ä»–'], \n",
    "    'WindDir9am': ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'å…¶ä»–'], \n",
    "    'Month': [str(i) for i in range(1, 13)]\n",
    "}\n",
    "\n",
    "# --- è¼‰å…¥æ¨¡å‹èˆ‡å·¥å…· ---\n",
    "try:\n",
    "    # è¼‰å…¥æ¨¡å‹\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    # è¼‰å…¥ StandardScaler\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    # è¼‰å…¥è¨“ç·´æ™‚ä½¿ç”¨çš„ç‰¹å¾µåç¨±åˆ—è¡¨ (åŒ…å«æ‰€æœ‰æ•¸å€¼å’Œ One-Hot ç‰¹å¾µ)\n",
    "    model_features = joblib.load(FEATURES_PATH)\n",
    "    \n",
    "    print(\"âœ… æ¨¡å‹èˆ‡å·¥å…·è¼‰å…¥æˆåŠŸï¼\")\n",
    "    print(f\"æ¨¡å‹é æœŸ {len(model_features)} å€‹ç‰¹å¾µã€‚\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è¼‰å…¥æ¨¡å‹æˆ–å·¥å…·æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "    print(\"è«‹ç¢ºä¿ 'random_forest_final_model.pth' å’Œ 'model_artifacts/' è³‡æ–™å¤¾å­˜åœ¨ä¸”åŒ…å«å¿…è¦çš„æª”æ¡ˆã€‚\")\n",
    "    sys.exit(1) # å¦‚æœè¼‰å…¥å¤±æ•—å‰‡çµ‚æ­¢ç¨‹å¼\n",
    "\n",
    "# ==============================================================================\n",
    "# å‡½æ•¸: ç²å–ç”¨æˆ¶è¼¸å…¥ (CLI äº¤äº’)\n",
    "# ==============================================================================\n",
    "def get_user_input():\n",
    "    \"\"\"äº¤äº’å¼ç²å–ç”¨æˆ¶è¼¸å…¥ã€‚\"\"\"\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"  è«‹è¼¸å…¥æ°£è±¡æ•¸æ“šé€²è¡Œé™é›¨é æ¸¬  \")\n",
    "    print(\"=\"*40)\n",
    "    raw_inputs = []\n",
    "\n",
    "    # 1. æ•¸å€¼ç‰¹å¾µè¼¸å…¥\n",
    "    print(\"\\n--- æ•¸å€¼ç‰¹å¾µè¼¸å…¥ ---\")\n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        while True:\n",
    "            try:\n",
    "                # æç¤ºç”¨æˆ¶è¼¸å…¥\n",
    "                user_input = input(f\"è«‹è¼¸å…¥ {feature} (æ•¸å€¼): \").strip()\n",
    "                if not user_input:\n",
    "                     # å…è¨±ç©ºè¼¸å…¥ä¸¦ä½¿ç”¨ Pandas é è¨­çš„ NaN è™•ç† (ä½†ç”±æ–¼æˆ‘å€‘åˆå§‹åŒ–ç‚º 0ï¼Œé€™è£¡å¿…é ˆè¦æ±‚è¼¸å…¥)\n",
    "                     raise ValueError(\"è¼¸å…¥ä¸èƒ½ç‚ºç©ºã€‚\")\n",
    "                     \n",
    "                value = float(user_input)\n",
    "                raw_inputs.append(value)\n",
    "                break\n",
    "            except ValueError:\n",
    "                print(\"è¼¸å…¥éŒ¯èª¤ï¼šè«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—ã€‚\")\n",
    "\n",
    "    # 2. åˆ†é¡ç‰¹å¾µè¼¸å…¥\n",
    "    print(\"\\n--- åˆ†é¡ç‰¹å¾µè¼¸å…¥ ---\")\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        choices = CATEGORY_CHOICES.get(feature, [])\n",
    "        while True:\n",
    "            if feature == 'Location':\n",
    "                print(f\"åœ°é»é¸é …ç¯„ä¾‹: {', '.join(choices[:5])}...\")\n",
    "            elif feature == 'WindDir9am':\n",
    "                print(f\"é¢¨å‘é¸é …ç¯„ä¾‹: {', '.join(choices[:8])}\")\n",
    "            elif feature == 'Month':\n",
    "                print(f\"æœˆä»½é¸é …: {', '.join(choices)}\")\n",
    "            \n",
    "            user_input = input(f\"è«‹è¼¸å…¥ {feature}: \").strip()\n",
    "            \n",
    "            if user_input:\n",
    "                # é‡å° Monthï¼Œç¢ºä¿æ˜¯ 1-12 çš„æ•´æ•¸\n",
    "                if feature == 'Month':\n",
    "                    try:\n",
    "                        month_int = int(user_input)\n",
    "                        if 1 <= month_int <= 12:\n",
    "                            raw_inputs.append(month_int)\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"è¼¸å…¥éŒ¯èª¤ï¼šæœˆä»½å¿…é ˆåœ¨ 1 åˆ° 12 ä¹‹é–“ã€‚\")\n",
    "                    except ValueError:\n",
    "                        print(\"è¼¸å…¥éŒ¯èª¤ï¼šæœˆä»½å¿…é ˆæ˜¯æ•´æ•¸ã€‚\")\n",
    "                else:\n",
    "                    raw_inputs.append(user_input)\n",
    "                    break\n",
    "            else:\n",
    "                print(\"è¼¸å…¥ä¸èƒ½ç‚ºç©ºï¼Œè«‹é‡æ–°è¼¸å…¥ã€‚\")\n",
    "                \n",
    "    return raw_inputs\n",
    "\n",
    "# ==============================================================================\n",
    "# å‡½æ•¸: åŸ·è¡Œé æ¸¬ä¸¦è¼¸å‡ºçµæœ\n",
    "# ==============================================================================\n",
    "def predict_cli(raw_inputs):\n",
    "    \"\"\"å°‡åŸå§‹è¼¸å…¥è½‰æ›ç‚ºæ¨¡å‹æ ¼å¼ï¼ŒåŸ·è¡Œé æ¸¬ï¼Œä¸¦è¼¸å‡ºçµæœã€‚\"\"\"\n",
    "    \n",
    "    # 1. æ ¹æ“š model_features åˆ—è¡¨ï¼Œæ§‹é€ ä¸€å€‹ç©ºçš„ DataFrame\n",
    "    input_df = pd.DataFrame(0.0, index=[0], columns=model_features)\n",
    "\n",
    "    # 2. å¡«å……æ•¸å€¼ç‰¹å¾µ\n",
    "    num_inputs = raw_inputs[:len(NUMERICAL_FEATURES)]\n",
    "    for feature, value in zip(NUMERICAL_FEATURES, num_inputs):\n",
    "        # åƒ…å¡«å……æ¨¡å‹ä¸­å¯¦éš›å­˜åœ¨çš„ç‰¹å¾µ\n",
    "        if feature in input_df.columns:\n",
    "            input_df[feature].iloc[0] = value\n",
    "            \n",
    "    # 3. å¡«å……åˆ†é¡ç‰¹å¾µ (One-Hot Encoding è½‰æ›)\n",
    "    cat_inputs = raw_inputs[len(NUMERICAL_FEATURES):]\n",
    "    \n",
    "    # Location\n",
    "    location_input = cat_inputs[0]\n",
    "    ohe_col_location = f'Location_{location_input}'\n",
    "    if ohe_col_location in input_df.columns:\n",
    "        input_df[ohe_col_location].iloc[0] = 1.0\n",
    "    \n",
    "    # WindDir9am\n",
    "    winddir9am_input = cat_inputs[1]\n",
    "    ohe_col_winddir = f'WindDir9am_{winddir9am_input}'\n",
    "    if ohe_col_winddir in input_df.columns:\n",
    "        input_df[ohe_col_winddir].iloc[0] = 1.0\n",
    "    \n",
    "    # Month\n",
    "    month_input = cat_inputs[2]\n",
    "    ohe_col_month = f'Month_{int(month_input)}'\n",
    "    if ohe_col_month in input_df.columns:\n",
    "        input_df[ohe_col_month].iloc[0] = 1.0\n",
    "        \n",
    "    # 4. æ¨™æº–åŒ– (Scaling)\n",
    "    input_scaled = pd.DataFrame(scaler.transform(input_df), columns=model_features)\n",
    "\n",
    "    # 5. é€²è¡Œé æ¸¬\n",
    "    proba = model.predict_proba(input_scaled)[:, 1][0]\n",
    "    \n",
    "    # ğŸ¯ ä½¿ç”¨æœ€ä½³é–¾å€¼é€²è¡Œæœ€çµ‚æ±ºç­–\n",
    "    prediction = 1 if proba >= OPTIMAL_THRESHOLD else 0\n",
    "    \n",
    "    # 6. æ ¼å¼åŒ–è¼¸å‡º\n",
    "    prediction_label = \"âœ… æ˜¯ (é æ¸¬æœƒä¸‹é›¨)\" if prediction == 1 else \"âŒ å¦ (é æ¸¬ä¸æœƒä¸‹é›¨)\"\n",
    "    confidence = f\"{proba * 100:.2f}%\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"        âœ¨ é æ¸¬çµæœ âœ¨       \")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"é æ¸¬æ©Ÿç‡ (æ˜æ—¥ä¸‹é›¨, P=1)ï¼š {confidence}\")\n",
    "    print(f\"æœ€çµ‚åˆ¤æ–· (é–¾å€¼ {OPTIMAL_THRESHOLD}): {prediction_label}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "# ==============================================================================\n",
    "# ä¸»ç¨‹å¼åŸ·è¡Œå€å¡Š\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        try:\n",
    "            user_inputs = get_user_input()\n",
    "            predict_cli(user_inputs)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}\")\n",
    "            print(\"è«‹æª¢æŸ¥æ‚¨çš„è¼¸å…¥æ ¼å¼æ˜¯å¦æ­£ç¢ºã€‚\")\n",
    "        \n",
    "        # è©¢å•ç”¨æˆ¶æ˜¯å¦ç¹¼çºŒ\n",
    "        print(\"\\næ˜¯å¦è¦é€²è¡Œæ–°çš„é æ¸¬ï¼Ÿ (è¼¸å…¥ 'y' ç¹¼çºŒï¼Œå…¶ä»–ä»»æ„éµçµæŸ)\")\n",
    "        continue_query = input().strip().lower()\n",
    "        if continue_query != 'y':\n",
    "            break\n",
    "\n",
    "    print(\"\\né æ¸¬ç¨‹å¼çµæŸã€‚æ„Ÿè¬æ‚¨çš„ä½¿ç”¨ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720361df-8ab0-447d-b489-7fb2bdf5cd06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
